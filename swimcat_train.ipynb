{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"swimcat_train.ipynb","provenance":[],"authorship_tag":"ABX9TyND1tCgF92lb3QOcArB4YvU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ga0MS5BE8Lv0"},"source":["## **Imports and Setup**"]},{"cell_type":"code","metadata":{"id":"IydHhEnraknB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609740781734,"user_tz":0,"elapsed":4097,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}},"outputId":"e49d0ea6-90d3-4628-f715-eacf9a8660f9"},"source":["import os\r\n","import numpy as np\r\n","import cv2\r\n","import tensorflow as tf\r\n","from tensorflow.keras.models import Sequential, load_model\r\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\r\n","from tensorflow.keras.optimizers import Adam\r\n","from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, LambdaCallback\r\n","import matplotlib.pyplot as plt\r\n","from sklearn.model_selection import train_test_split\r\n","import random\r\n","import json\r\n","#from copy import copy\r\n","from tensorflow.keras.applications import VGG16 \r\n","\r\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y3G7AUhGxx5A"},"source":["## **Mount Notebook on Google Drive**\r\n","Mount and give read/write access directly to Google Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zFLEVYxByPTn","executionInfo":{"status":"ok","timestamp":1609740805263,"user_tz":0,"elapsed":21218,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}},"outputId":"41c1ee06-3512-42eb-980e-c7cada552c41"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"haXhhbd0zR_p"},"source":["## **Read & Pre-Process Data**"]},{"cell_type":"markdown","metadata":{"id":"MWO49rLmnrvK"},"source":["Read Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JnctglvRzRgz","executionInfo":{"status":"ok","timestamp":1609740983529,"user_tz":0,"elapsed":177028,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}},"outputId":"828344c6-8a9b-4f74-9980-3c68ee0522c3"},"source":["data_folder = '/content/drive/My Drive/Colab Notebooks/Cloud Classification/data/swimcat/'\r\n","\r\n","def load_images_from_folder(folder):\r\n","  images = []\r\n","  for filename in os.listdir(folder):\r\n","    img = cv2.imread(os.path.join(folder,filename))\r\n","    if img is not None:\r\n","      images.append(img.astype('uint8')) # Store in 'uint8' format to reduce both memory and time complexity\r\n","  return images\r\n","\r\n","def read_swimcat_data(data_folder):\r\n","  class_0 = load_images_from_folder(data_folder+'A-sky/images/')\r\n","  print('Class 0: ', len(class_0), class_0[0].shape)\r\n","  class_1 = load_images_from_folder(data_folder+'B-pattern/images/')\r\n","  print('Class 1: ', len(class_1), class_1[0].shape)\r\n","  class_2 = load_images_from_folder(data_folder+'C-thick-dark/images/')\r\n","  print('Class 2: ', len(class_2), class_2[0].shape)\r\n","  class_3 = load_images_from_folder(data_folder+'D-thick-white/images/')\r\n","  print('Class 3: ', len(class_3), class_3[0].shape)\r\n","  class_4 = load_images_from_folder(data_folder+'E-veil/images/')\r\n","  print('Class 4: ', len(class_4), class_4[0].shape)\r\n","  return class_0, class_1, class_2, class_3, class_4\r\n","\r\n","class_0, class_1, class_2, class_3, class_4 = read_swimcat_data(data_folder)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Class 0:  224 (125, 125, 3)\n","Class 1:  89 (125, 125, 3)\n","Class 2:  251 (125, 125, 3)\n","Class 3:  135 (125, 125, 3)\n","Class 4:  85 (125, 125, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K-M8vfUm3fzN"},"source":["Function to Augment Images"]},{"cell_type":"code","metadata":{"id":"ZFIkAD8i3k8D","executionInfo":{"status":"ok","timestamp":1609740988274,"user_tz":0,"elapsed":1318,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}}},"source":["# Creates 144 tiled images from 1 original image - i.e. augment by a factor of 144\r\n","def augment_imageData(imageList, ori_row=125, ori_col=125, tiling=0.2):\r\n","  augmentedImages = []\r\n","  for image in imageList:\r\n","    if (not image.shape[0]==ori_row) or (not image.shape[1]==ori_col):\r\n","      print(image.shape)\r\n","      raise Exception(\"Dimensions of provided image doesn't match with specified dimensions!\")\r\n","    # tiledImages = [leftTopCorner, centerTop, rightTopCorner,\r\n","    #                centerLeft, center, centerRight,\r\n","    #                leftBottomCorner, centerBottom, rightBottomCorner]\r\n","    tiledImages = [\r\n","                   image[0:int(ori_row*(1-tiling)), 0:int(ori_col*(1-tiling))],\r\n","                   image[0:int(ori_row*(1-tiling)), int(ori_col*(tiling/2)):int(ori_col*(1-(tiling/2)))],\r\n","                   image[0:int(ori_row*(1-tiling)), int(ori_col*tiling):ori_col],\r\n","                   image[int(ori_row*(tiling/2)):int(ori_row*(1-(tiling/2))), 0:int(ori_col*(1-tiling))],\r\n","                   image[int(ori_row*(tiling/2)):int(ori_row*(1-(tiling/2))), int(ori_col*(tiling/2)):int(ori_col*(1-(tiling/2)))],\r\n","                   image[int(ori_row*(tiling/2)):int(ori_row*(1-(tiling/2))), int(ori_col*tiling):ori_col],\r\n","                   image[int(ori_row*tiling):ori_row, 0:int(ori_col*(1-tiling))],\r\n","                   image[int(ori_row*tiling):ori_row, int(ori_col*(tiling/2)):int(ori_col*(1-(tiling/2)))],\r\n","                   image[int(ori_row*tiling):ori_row, int(ori_col*tiling):ori_col]\r\n","                  ]\r\n","    frtImages = []\r\n","    for tiledImage in tiledImages:\r\n","      if (not tiledImage.shape[0]==ori_row*(1-tiling)) or (not tiledImage.shape[1]==ori_col*(1-tiling)):\r\n","        raise Exception(\"Shape inconsistency error during tiling operation!\")\r\n","      flippedImages = [\r\n","                       tiledImage, # Original\r\n","                       cv2.flip(src=tiledImage, flipCode=0).astype('uint8'), # Vertical Flip - around x-axis\r\n","                       cv2.flip(src=tiledImage, flipCode=1).astype('uint8'), # Horizontal Flip - around y-axis\r\n","                       cv2.flip(src=tiledImage, flipCode=-1).astype('uint8') # Both type of FLips\r\n","                      ]\r\n","      frImages = []\r\n","      for flippedImage in flippedImages:\r\n","        if (not flippedImage.shape[0]==ori_row*(1-tiling)) or (not flippedImage.shape[1]==ori_col*(1-tiling)):\r\n","          raise Exception(\"Shape inconsistency error during tiling operation!\")\r\n","        rotatedImages = [\r\n","                         flippedImage, # Original\r\n","                         cv2.rotate(flippedImage, cv2.ROTATE_90_COUNTERCLOCKWISE).astype('uint8'), # 90 degree rotation\r\n","                         cv2.rotate(flippedImage, cv2.ROTATE_180).astype('uint8'), # 180 degree rotation\r\n","                         cv2.rotate(flippedImage, cv2.ROTATE_90_CLOCKWISE).astype('uint8') # 270 degree rotation\r\n","                        ]\r\n","        frImages.extend(rotatedImages)\r\n","      frtImages.extend(frImages)\r\n","    augmentedImages.extend(frtImages)\r\n","  return augmentedImages\r\n","\r\n","# Function to extract center tile - for validation and testing purposes\r\n","def extract_center_tile(imageList, ori_row=125, ori_col=125, tiling=0.2):\r\n","  centerTiles = [None]*len(imageList)\r\n","  for id, image in enumerate(imageList):\r\n","    centerTiles[id] = image[int(ori_row*(tiling/2)):int(ori_row*(1-(tiling/2))), int(ori_col*(tiling/2)):int(ori_col*(1-(tiling/2)))]\r\n","    if (not centerTiles[id].shape[0]==ori_row*(1-tiling)) or (not centerTiles[id].shape[1]==ori_col*(1-tiling)):\r\n","        raise Exception(\"Shape inconsistency error during tiling operation!\")\r\n","  return centerTiles"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0rcddrUnwcS"},"source":["Create Train, Validation, and Test Splits"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHUgRHlfh57h","executionInfo":{"status":"ok","timestamp":1609740996595,"user_tz":0,"elapsed":3812,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}},"outputId":"d9f3cc65-4d7a-4d81-97fa-593057ab51f8"},"source":["def create_splits(class_0, class_1, class_2, class_3, class_4, random_state=42, test_size=0.2):\r\n","  #---------------------------------------\r\n","  # Create Class-wise Initial Splits\r\n","  #---------------------------------------\r\n","  # Split Class 0\r\n","  X_train_complete_0, X_test_0, y_train_complete_0, y_test_0 = train_test_split(class_0, [np.array([1, 0, 0, 0, 0])]*len(class_0), random_state=random_state, test_size=test_size)\r\n","  X_train_0, X_val_0, y_train_0, y_val_0 = train_test_split(X_train_complete_0, y_train_complete_0, random_state=random_state, test_size=test_size)\r\n","  # Split Class 1\r\n","  X_train_complete_1, X_test_1, y_train_complete_1, y_test_1 = train_test_split(class_1, [np.array([0, 1, 0, 0, 0])]*len(class_1), random_state=random_state, test_size=test_size)\r\n","  X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(X_train_complete_1, y_train_complete_1, random_state=random_state, test_size=test_size)\r\n","  # Split Class 2\r\n","  X_train_complete_2, X_test_2, y_train_complete_2, y_test_2 = train_test_split(class_2, [np.array([0, 0, 1, 0, 0])]*len(class_2), random_state=random_state, test_size=test_size)\r\n","  X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(X_train_complete_2, y_train_complete_2, random_state=random_state, test_size=test_size)\r\n","  # Split Class 3\r\n","  X_train_complete_3, X_test_3, y_train_complete_3, y_test_3 = train_test_split(class_3, [np.array([0, 0, 0, 1, 0])]*len(class_3), random_state=random_state, test_size=test_size)\r\n","  X_train_3, X_val_3, y_train_3, y_val_3 = train_test_split(X_train_complete_3, y_train_complete_3, random_state=random_state, test_size=test_size)\r\n","  # Split Class 4\r\n","  X_train_complete_4, X_test_4, y_train_complete_4, y_test_4 = train_test_split(class_4, [np.array([0, 0, 0, 0, 1])]*len(class_4), random_state=random_state, test_size=test_size)\r\n","  X_train_4, X_val_4, y_train_4, y_val_4 = train_test_split(X_train_complete_4, y_train_complete_4, random_state=random_state, test_size=test_size)\r\n","  #---------------------------------------\r\n","  # Augment Training Images\r\n","  #---------------------------------------\r\n","  # Augmenting Class 0\r\n","  X_train_0_aug = augment_imageData(X_train_0)\r\n","  y_train_0_aug = [np.array([1, 0, 0, 0, 0])]*len(X_train_0_aug)\r\n","  X_val_0_tiled = extract_center_tile(X_val_0)\r\n","  X_test_0_tiled = extract_center_tile(X_test_0)\r\n","  # Augmenting Class 1\r\n","  X_train_1_aug = augment_imageData(X_train_1)\r\n","  y_train_1_aug = [np.array([0, 1, 0, 0, 0])]*len(X_train_1_aug)\r\n","  X_val_1_tiled = extract_center_tile(X_val_1)\r\n","  X_test_1_tiled = extract_center_tile(X_test_1)\r\n","  # Augmenting Class 2\r\n","  X_train_2_aug = augment_imageData(X_train_2)\r\n","  y_train_2_aug = [np.array([0, 0, 1, 0, 0])]*len(X_train_2_aug)\r\n","  X_val_2_tiled = extract_center_tile(X_val_2)\r\n","  X_test_2_tiled = extract_center_tile(X_test_2)\r\n","  # Augmenting Class 3\r\n","  X_train_3_aug = augment_imageData(X_train_3)\r\n","  y_train_3_aug = [np.array([0, 0, 0, 1, 0])]*len(X_train_3_aug)\r\n","  X_val_3_tiled = extract_center_tile(X_val_3)\r\n","  X_test_3_tiled = extract_center_tile(X_test_3)\r\n","  # Augmenting Class 4\r\n","  X_train_4_aug = augment_imageData(X_train_4)\r\n","  y_train_4_aug = [np.array([0, 0, 0, 0, 1])]*len(X_train_4_aug)\r\n","  X_val_4_tiled = extract_center_tile(X_val_4)\r\n","  X_test_4_tiled = extract_center_tile(X_test_4)\r\n","  #---------------------------------------\r\n","  # Create Final Splits\r\n","  #---------------------------------------\r\n","  # Complete Train Split (NS - abbreviation for NOT SHUFFLED)\r\n","  X_train_NS = X_train_0_aug + X_train_1_aug + X_train_2_aug + X_train_3_aug + X_train_4_aug\r\n","  y_train_NS = y_train_0_aug + y_train_1_aug + y_train_2_aug + y_train_3_aug + y_train_4_aug\r\n","  temp = list(zip(X_train_NS, y_train_NS))\r\n","  random.shuffle(temp)\r\n","  X_train, y_train = zip(*temp)\r\n","  X_val_NS = X_val_0_tiled + X_val_1_tiled + X_val_2_tiled + X_val_3_tiled + X_val_4_tiled\r\n","  y_val_NS = y_val_0 + y_val_1 + y_val_2 + y_val_3 + y_val_4\r\n","  temp = list(zip(X_val_NS, y_val_NS))\r\n","  random.shuffle(temp)\r\n","  X_val, y_val = zip(*temp)\r\n","  X_test_NS = X_test_0_tiled + X_test_1_tiled + X_test_2_tiled + X_test_3_tiled + X_test_4_tiled\r\n","  y_test_NS = y_test_0 + y_test_1 + y_test_2 + y_test_3 + y_test_4\r\n","  temp = list(zip(X_test_NS, y_test_NS))\r\n","  random.shuffle(temp)\r\n","  X_test, y_test = zip(*temp)\r\n","  # Return splits\r\n","  return X_train, y_train, X_val, y_val, X_test, y_test\r\n","\r\n","X_train, y_train, X_val, y_val, X_test, y_test = create_splits(class_0, class_1, class_2, class_3, class_4)\r\n","print('Number of images in Training Set = ', len(X_train))\r\n","print('Number of images in Validation Set = ', len(X_val))\r\n","print('Number of images in Test Set = ', len(X_test))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Number of images in Training Set =  71856\n","Number of images in Validation Set =  127\n","Number of images in Test Set =  158\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ufvzHNbg8lv7"},"source":["## **Training Deep-CNN Model**"]},{"cell_type":"markdown","metadata":{"id":"kxmsOuSK_G58"},"source":["Declare Model Architecture and Hyper-Parameter Settings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KUcA9DF58xAq","executionInfo":{"status":"ok","timestamp":1609741008539,"user_tz":0,"elapsed":7538,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}},"outputId":"17036ae4-40dc-408f-bdd7-1d02f096be71"},"source":["results_folder = \"/content/drive/My Drive/Colab Notebooks/Cloud Classification/results/\"\r\n","\r\n","input_shape = X_train[0].shape\r\n","num_classes = 5\r\n","\r\n","NB_EPOCHS = 100\r\n","BATCH_SIZE = 64\r\n","LEARNING_RATE = 1e-7\r\n","\r\n","conv_base = VGG16(weights='imagenet',include_top=False,input_shape=input_shape)\r\n","model = Sequential()\r\n","model.add(conv_base)\r\n","model.add (Flatten())\r\n","model.add(Dense(256,activation=\"relu\"))\r\n","model.add(Dense(num_classes,activation=\"softmax\"))\r\n","model.summary()\r\n","opt = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=None, decay=1e-6, amsgrad=False)\r\n","model.compile(loss=\"categorical_crossentropy\",\r\n","              optimizer=opt,\r\n","              metrics=[\"accuracy\"])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 3, 3, 512)         14714688  \n","_________________________________________________________________\n","flatten (Flatten)            (None, 4608)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               1179904   \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 5)                 1285      \n","=================================================================\n","Total params: 15,895,877\n","Trainable params: 15,895,877\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UBnMJRFY_MiH"},"source":["Start Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jwMlU3Ux_Opz","executionInfo":{"status":"ok","timestamp":1609632956090,"user_tz":0,"elapsed":5758263,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}},"outputId":"321908d9-73a2-42bb-bc12-ac4867dbc527"},"source":["checkpoint = ModelCheckpoint(results_folder+\"checkpointWeightsBest.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\r\n","csv_logger = CSVLogger(results_folder+\"model_history_log.csv\", append=False)\r\n","\r\n","json_log = open(results_folder+'loss_log.json', mode='wt', buffering=1)\r\n","json_logging_callback = LambdaCallback(\r\n","    on_epoch_end=lambda epoch, logs: json_log.write(\r\n","        json.dumps({'epoch': epoch, 'loss': logs['loss'], 'val_loss': logs['val_loss'], 'acc': logs['accuracy'], 'val_acc': logs['val_accuracy']}) + '\\n'),\r\n","    on_train_end=lambda logs: json_log.close()\r\n",")\r\n","\r\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\r\n","\r\n","callbacks_list = [checkpoint, csv_logger, json_logging_callback, es]\r\n","\r\n","history = model.fit(np.asarray(X_train), np.asarray(y_train), validation_data=(np.asarray(X_val), np.asarray(y_val)), epochs=NB_EPOCHS, callbacks=callbacks_list)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","2246/2246 [==============================] - 234s 103ms/step - loss: 2.7431 - accuracy: 0.5187 - val_loss: 0.3577 - val_accuracy: 0.8898\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.88976, saving model to /content/drive/My Drive/Colab Notebooks/Cloud Classification/results/checkpointWeightsBest.hdf5\n","Epoch 2/100\n","2246/2246 [==============================] - 229s 102ms/step - loss: 0.2039 - accuracy: 0.9341 - val_loss: 0.1157 - val_accuracy: 0.9606\n","\n","Epoch 00002: val_accuracy improved from 0.88976 to 0.96063, saving model to /content/drive/My Drive/Colab Notebooks/Cloud Classification/results/checkpointWeightsBest.hdf5\n","Epoch 3/100\n","2246/2246 [==============================] - 229s 102ms/step - loss: 0.0614 - accuracy: 0.9808 - val_loss: 0.0678 - val_accuracy: 0.9606\n","\n","Epoch 00003: val_accuracy did not improve from 0.96063\n","Epoch 4/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.0564 - val_accuracy: 0.9685\n","\n","Epoch 00004: val_accuracy improved from 0.96063 to 0.96850, saving model to /content/drive/My Drive/Colab Notebooks/Cloud Classification/results/checkpointWeightsBest.hdf5\n","Epoch 5/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.0551 - val_accuracy: 0.9606\n","\n","Epoch 00005: val_accuracy did not improve from 0.96850\n","Epoch 6/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.0623 - val_accuracy: 0.9606\n","\n","Epoch 00006: val_accuracy did not improve from 0.96850\n","Epoch 7/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.0550 - val_accuracy: 0.9685\n","\n","Epoch 00007: val_accuracy did not improve from 0.96850\n","Epoch 8/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 9.7710e-04 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9764\n","\n","Epoch 00008: val_accuracy improved from 0.96850 to 0.97638, saving model to /content/drive/My Drive/Colab Notebooks/Cloud Classification/results/checkpointWeightsBest.hdf5\n","Epoch 9/100\n","2246/2246 [==============================] - 229s 102ms/step - loss: 4.3807e-04 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9764\n","\n","Epoch 00009: val_accuracy did not improve from 0.97638\n","Epoch 10/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 1.9120e-04 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.9764\n","\n","Epoch 00010: val_accuracy did not improve from 0.97638\n","Epoch 11/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 8.8603e-05 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9764\n","\n","Epoch 00011: val_accuracy did not improve from 0.97638\n","Epoch 12/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 4.2285e-05 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 0.9843\n","\n","Epoch 00012: val_accuracy improved from 0.97638 to 0.98425, saving model to /content/drive/My Drive/Colab Notebooks/Cloud Classification/results/checkpointWeightsBest.hdf5\n","Epoch 13/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 1.9468e-05 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9764\n","\n","Epoch 00013: val_accuracy did not improve from 0.98425\n","Epoch 14/100\n","2246/2246 [==============================] - 229s 102ms/step - loss: 9.3079e-06 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9764\n","\n","Epoch 00014: val_accuracy did not improve from 0.98425\n","Epoch 15/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 3.5183e-06 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9843\n","\n","Epoch 00015: val_accuracy did not improve from 0.98425\n","Epoch 16/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 1.8150e-06 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9921\n","\n","Epoch 00016: val_accuracy improved from 0.98425 to 0.99213, saving model to /content/drive/My Drive/Colab Notebooks/Cloud Classification/results/checkpointWeightsBest.hdf5\n","Epoch 17/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 6.3517e-07 - accuracy: 1.0000 - val_loss: 0.0548 - val_accuracy: 0.9921\n","\n","Epoch 00017: val_accuracy did not improve from 0.99213\n","Epoch 18/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 2.9121e-07 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9921\n","\n","Epoch 00018: val_accuracy did not improve from 0.99213\n","Epoch 19/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 1.5379e-07 - accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9921\n","\n","Epoch 00019: val_accuracy did not improve from 0.99213\n","Epoch 20/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 7.4488e-08 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9921\n","\n","Epoch 00020: val_accuracy did not improve from 0.99213\n","Epoch 21/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 3.8721e-08 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9921\n","\n","Epoch 00021: val_accuracy did not improve from 0.99213\n","Epoch 22/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 2.2331e-08 - accuracy: 1.0000 - val_loss: 0.0701 - val_accuracy: 0.9921\n","\n","Epoch 00022: val_accuracy did not improve from 0.99213\n","Epoch 23/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 1.5656e-08 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9921\n","\n","Epoch 00023: val_accuracy did not improve from 0.99213\n","Epoch 24/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 1.0689e-08 - accuracy: 1.0000 - val_loss: 0.0757 - val_accuracy: 0.9921\n","\n","Epoch 00024: val_accuracy did not improve from 0.99213\n","Epoch 25/100\n","2246/2246 [==============================] - 230s 102ms/step - loss: 8.1640e-09 - accuracy: 1.0000 - val_loss: 0.0774 - val_accuracy: 0.9921\n","\n","Epoch 00025: val_accuracy did not improve from 0.99213\n","Epoch 00025: early stopping\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UtbGJV6ELZcE"},"source":["Resume Training"]},{"cell_type":"code","metadata":{"id":"AaA_OvSBLbbE"},"source":["checkpoint = ModelCheckpoint(results_folder+\"checkpointWeightsBest.hdf5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\r\n","csv_logger = CSVLogger(results_folder+\"model_history_log.csv\", append=True)\r\n","\r\n","json_log = open(results_folder+'loss_log.json', mode='wt', buffering=1)\r\n","json_logging_callback = LambdaCallback(\r\n","    on_epoch_end=lambda epoch, logs: json_log.write(\r\n","        json.dumps({'epoch': epoch, 'loss': logs['loss'], 'val_loss': logs['val_loss'], 'acc': logs['accuracy'], 'val_acc': logs['val_accuracy']}) + '\\n'),\r\n","    on_train_end=lambda logs: json_log.close()\r\n",")\r\n","\r\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\r\n","\r\n","callbacks_list = [checkpoint, csv_logger, json_logging_callback, es]\r\n","\r\n","model = load_model(results_folder+\"checkpointWeightsBest.hdf5\")\r\n","\r\n","history = model.fit(np.asarray(X_train), np.asarray(y_train), validation_data=(np.asarray(X_val), np.asarray(y_val)), epochs=NB_EPOCHS, callbacks=callbacks_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gcjJ0auh4ZeL"},"source":["Plot and Save Training Curves"]},{"cell_type":"code","metadata":{"id":"nYi8xmA44dnd","executionInfo":{"status":"ok","timestamp":1609741024387,"user_tz":0,"elapsed":1671,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}}},"source":["training_logs = {'epoch'        : np.array([]),\r\n","                 'loss'         : np.array([]),\r\n","                 'val_loss'     : np.array([]),\r\n","                 'accuracy'     : np.array([]),\r\n","                 'val_accuracy' : np.array([])}\r\n","with open(results_folder+'loss_log.json') as logsJSON:\r\n","  lines = logsJSON.readlines()\r\n","  for line in lines:\r\n","    logsDict = json.loads(line)\r\n","    training_logs['epoch'] = np.append(training_logs['epoch'], logsDict['epoch'])\r\n","    training_logs['loss'] = np.append(training_logs['loss'], logsDict['loss'])\r\n","    training_logs['val_loss'] = np.append(training_logs['val_loss'], logsDict['val_loss'])\r\n","    training_logs['accuracy'] = np.append(training_logs['accuracy'], logsDict['acc'])\r\n","    training_logs['val_accuracy'] = np.append(training_logs['val_accuracy'], logsDict['val_acc'])\r\n","\r\n","# Plot Loss Curves\r\n","plt.figure(figsize=(10, 6))\r\n","plt.rc('font', size=17)         # controls default text sizes\r\n","plt.rc('axes', titlesize=19)    # fontsize of the axes title\r\n","plt.rc('axes', labelsize=19)    # fontsize of the x and y labels\r\n","plt.rc('xtick', labelsize=17)   # fontsize of the tick labels\r\n","plt.rc('ytick', labelsize=17)   # fontsize of the tick labels\r\n","plt.rc('legend', fontsize=17)   # legend fontsize\r\n","plt.rc('figure', titlesize=19)  # fontsize of the figure title\r\n","plt.plot(training_logs['epoch'], training_logs['loss'], 'r--', label='Training Loss')\r\n","plt.plot(training_logs['epoch'], training_logs['val_loss'], 'b--', label='Validation Loss')\r\n","plt.xlabel('Epoch')\r\n","plt.ylabel('Loss')\r\n","plt.legend()\r\n","plt.grid(True)\r\n","plt.savefig(results_folder+'training_loss_characteristics.pdf', bbox_inches = 'tight', pad_inches = 0.05)\r\n","plt.close()\r\n","\r\n","# Plot Accuracy Curves\r\n","plt.figure(figsize=(10, 6))\r\n","plt.rc('font', size=15)         # controls default text sizes\r\n","plt.rc('axes', titlesize=17)    # fontsize of the axes title\r\n","plt.rc('axes', labelsize=17)    # fontsize of the x and y labels\r\n","plt.rc('xtick', labelsize=15)   # fontsize of the tick labels\r\n","plt.rc('ytick', labelsize=15)   # fontsize of the tick labels\r\n","plt.rc('legend', fontsize=15)   # legend fontsize\r\n","plt.rc('figure', titlesize=17)  # fontsize of the figure title\r\n","plt.plot(training_logs['epoch'], training_logs['accuracy'], 'r--', label='Training Accuracy')\r\n","plt.plot(training_logs['epoch'], training_logs['val_accuracy'], 'b--', label='Validation Accuracy')\r\n","plt.xlabel('Epoch')\r\n","plt.ylabel('Accuracy')\r\n","plt.legend()\r\n","plt.grid(True)\r\n","plt.savefig(results_folder+'training_accuracy_characteristics.pdf', bbox_inches = 'tight', pad_inches = 0.05)\r\n","plt.close()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s9eayPCkAGJF"},"source":["Caluculate Loss and Accuracy on Test Set"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2P0khCN1ANBQ","executionInfo":{"status":"ok","timestamp":1609741123345,"user_tz":0,"elapsed":88873,"user":{"displayName":"Mayank Jain","photoUrl":"","userId":"09883289633884974344"}},"outputId":"b7dc89e9-4ceb-4b27-c576-5bae9b4cf832"},"source":["model = load_model(results_folder+\"checkpointWeightsBest.hdf5\")\r\n","print(\"\\nEvaluating on training data...\")\r\n","results = model.evaluate(np.asarray(X_train), np.asarray(y_train), batch_size=BATCH_SIZE)\r\n","print(\"Train loss, Train accuracy:\", results)\r\n","print(\"\\nEvaluating on validation data...\")\r\n","results = model.evaluate(np.asarray(X_val), np.asarray(y_val))\r\n","print(\"Val loss, Val accuracy:\", results)\r\n","print(\"\\nEvaluating on test data...\")\r\n","results = model.evaluate(np.asarray(X_test), np.asarray(y_test))\r\n","print(\"Test loss, Test accuracy:\", results)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\n","Evaluating on training data...\n","1123/1123 [==============================] - 84s 68ms/step - loss: 7.9447e-07 - accuracy: 1.0000\n","Train loss, Train accuracy: [7.944732374198793e-07, 1.0]\n","\n","Evaluating on validation data...\n","4/4 [==============================] - 1s 180ms/step - loss: 0.0515 - accuracy: 0.9921\n","Val loss, Val accuracy: [0.05146964639425278, 0.9921259880065918]\n","\n","Evaluating on test data...\n","5/5 [==============================] - 1s 141ms/step - loss: 7.1612e-04 - accuracy: 1.0000\n","Test loss, Test accuracy: [0.0007161226239986718, 1.0]\n"],"name":"stdout"}]}]}